{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba95c93d",
   "metadata": {},
   "source": [
    "# Hugging Face tokenizers in Pyserini \n",
    "In this notebook we demonstrate how Hugging Face tokenizers can be leveraged in building BM25 indices with Anserini and Pyserini. The HF tokenizers serve as drop-in replacements for Lucene-native analyzers. More example of how to use Anserini with HF tokenizers can be found here https://github.com/castorini/anserini/blob/master/docs/regressions-msmarco-passage-hgf-wp.md."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb499eb1",
   "metadata": {},
   "source": [
    "We begin by importing all the necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c2ed4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from datasets.utils.py_utils import convert_file_size_to_int\n",
    "from pyserini.pyclass import autoclass\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import (\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    decoders,\n",
    "    Tokenizer,\n",
    ")\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92a2708",
   "metadata": {},
   "source": [
    "Subsequently we download and preprocess a dataset from the Hugging Face Hub and split it into shards. Check the Indexing notebook for more details of the steps below: https://github.com/huggingface/gaia/blob/main/notebooks/00-indexing.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27946c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/piktus_huggingface_co/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharding into 4 JSONL files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                                                          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0306ddbe964f36af20b4d52a135465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|████████████████████████████████████████████████████▌                                                                                                                                                             | 1/4 [00:00<00:00,  5.04it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9494d6247541499c8dbbf98d195d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                         | 2/4 [00:00<00:00,  5.48it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafd6e7480814c9381985f1d4e06ba3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                    | 3/4 [00:00<00:00,  5.45it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1bf6ac1c114b8d83fa15eda8e27105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.64it/s]\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset(\"imdb\", split=\"train\")\n",
    "dset = dset.add_column(\"id\", [str(i) for i in range(len(dset))])\n",
    "dset = dset.rename_column(\"text\", \"contents\")\n",
    "dset = dset.select_columns([\"id\", \"contents\"])\n",
    "\n",
    "shard_dir = f\"../shards/imdb\"\n",
    "max_shard_size = convert_file_size_to_int('10MB')\n",
    "dataset_nbytes = dset.data.nbytes\n",
    "num_shards = int(dataset_nbytes / max_shard_size) + 1\n",
    "num_shards = max(num_shards, 1)\n",
    "print(f\"Sharding into {num_shards} JSONL files.\")\n",
    "os.makedirs(shard_dir, exist_ok=True)\n",
    "for shard_index in tqdm(range(num_shards)):\n",
    "    shard = dset.shard(num_shards=num_shards, index=shard_index, contiguous=True)\n",
    "    shard.to_json(f\"{shard_dir}/docs-{shard_index:03d}.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8d8b6a",
   "metadata": {},
   "source": [
    "## Build the tokenizer\n",
    "Below we illustrate how one can train a custom BPE tokenizer for a given dataset and upload it to the Hugging Face Hub. Alternatively, one can also use any of the tokenizers already uploaded to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f696f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(\n",
    "    dataset, text_column_name, batch_size\n",
    "):  # Batch size has to be a multiple of the dataset size\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset.select(range(i, i + batch_size))[text_column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a14f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 25000\n",
    "\n",
    "tokenizer = Tokenizer(BPE())\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence([normalizers.NFKD(), normalizers.StripAccents()])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True, use_regex=True)\n",
    "tokenizer.decoder = decoders.ByteLevel(add_prefix_space=True, use_regex=True)\n",
    "\n",
    "trainer = BpeTrainer(vocab_size=VOCAB_SIZE, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3721961a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CPU times: user 30min 27s, sys: 3min 20s, total: 33min 48s\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer.train_from_iterator(batch_iterator(dset, \"contents\", 100), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892d1f3b",
   "metadata": {},
   "source": [
    "Finally, we wrap the tokenizer into a transformers tokenizer object to get access to the `push_to_hub` feature. We have already pre-uploaded a tokenizer build using the code above to the HF hub here: https://huggingface.co/spacerini/bpe-imdb-25k. Try out pushing a new tokenizer to the hub using your Hugging Face credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "023dcbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_model = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer, vocab_size=VOCAB_SIZE\n",
    ")\n",
    "# tokenizer_model.push_to_hub(\"your-username/bpe-imdb-25k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ed25de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġhello', 'Ġworld']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"hello world\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddda440",
   "metadata": {},
   "source": [
    "## Build the index with a HF tokenizer as the Analyzer\n",
    "In order to build an index using a HF tokenizer to tokenize the documents, we simply have to pass the `-analyzeWithHuggingFaceTokenizer` followed by the signature of the tokenizer in question as an argument to the indexer. E.g. we pass `spacerini/bpe-imdb-25k` to use the tokenizer trained above. Under the hood, the indexer will use the tokenizer to normalize the document and split it into terms (or tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b3f5a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "JIndexCollection = autoclass(\"io.anserini.index.IndexCollection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "36add330",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_args = [\n",
    "    \"-input\",\n",
    "    shard_dir,\n",
    "    \"-index\",\n",
    "    \"../indexes/bpe-imdb-25k\",\n",
    "    \"-collection\",\n",
    "    \"JsonCollection\",\n",
    "    \"-threads\",\n",
    "    \"28\",\n",
    "    \"-analyzeWithHuggingFaceTokenizer\",\n",
    "    \"spacerini/bpe-imdb-25k\",\n",
    "    \"-storePositions\",\n",
    "    \"-storeDocvectors\",\n",
    "    \"-storeContents\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3839c0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-17 15:22:52,552 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
      "2023-05-17 15:22:52,552 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
      "2023-05-17 15:22:52,552 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
      "2023-05-17 15:22:52,552 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: ../shards/imdb\n",
      "2023-05-17 15:22:52,553 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
      "2023-05-17 15:22:52,553 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
      "2023-05-17 15:22:52,553 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 28\n",
      "2023-05-17 15:22:52,553 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
      "2023-05-17 15:22:52,553 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
      "2023-05-17 15:22:52,555 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
      "2023-05-17 15:22:52,555 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
      "2023-05-17 15:22:52,555 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
      "2023-05-17 15:22:52,556 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
      "2023-05-17 15:22:52,556 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? true\n",
      "2023-05-17 15:22:52,556 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? false\n",
      "2023-05-17 15:22:52,556 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
      "2023-05-17 15:22:52,556 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
      "2023-05-17 15:22:52,556 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
      "2023-05-17 15:22:52,556 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: false\n",
      "2023-05-17 15:22:52,556 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: ../indexes/bpe-imdb-25k\n",
      "2023-05-17 15:22:52,557 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
      "2023-05-17 15:23:06,304 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 28 threads initialized.\n",
      "2023-05-17 15:23:06,305 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in ../shards/imdb\n",
      "2023-05-17 15:23:06,305 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 4 files found\n",
      "2023-05-17 15:23:06,306 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
      "2023-05-17 15:23:19,413 DEBUG [pool-4-thread-4] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - imdb/docs-003.jsonl: 6250 docs added.\n",
      "2023-05-17 15:23:19,485 DEBUG [pool-4-thread-3] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - imdb/docs-002.jsonl: 6250 docs added.\n",
      "2023-05-17 15:23:19,760 DEBUG [pool-4-thread-2] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - imdb/docs-000.jsonl: 6250 docs added.\n",
      "2023-05-17 15:23:19,780 DEBUG [pool-4-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - imdb/docs-001.jsonl: 6250 docs added.\n",
      "2023-05-17 15:23:21,005 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 25,000 documents indexed\n",
      "2023-05-17 15:23:21,005 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
      "2023-05-17 15:23:21,006 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:           25,000\n",
      "2023-05-17 15:23:21,006 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
      "2023-05-17 15:23:21,006 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
      "2023-05-17 15:23:21,006 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
      "2023-05-17 15:23:21,006 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
      "2023-05-17 15:23:21,007 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 25,000 documents indexed in 00:00:28\n"
     ]
    }
   ],
   "source": [
    "JIndexCollection.main(indexing_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acf9b84",
   "metadata": {},
   "source": [
    "# Using the index\n",
    "Follow the next tutorial to see how you can interact with your index:\n",
    "- searching: https://github.com/huggingface/gaia/blob/main/notebooks/02-searching.ipynb\n",
    "- analysis: https://github.com/huggingface/gaia/blob/main/notebooks/03-analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc63c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
